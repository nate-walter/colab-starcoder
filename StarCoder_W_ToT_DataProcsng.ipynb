{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyO6Z2IWb+maSnqmRX8IU8A4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1722adf14dcc4d4db3ea659bffef0186": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ab8aca607aab433a87bcd7766e8d1697",
              "IPY_MODEL_86f45e52c0a045c9887a484d713ffa08",
              "IPY_MODEL_fed9cf2bcc5d46d2a33adb4d2bd2cdfc"
            ],
            "layout": "IPY_MODEL_0ec11e17184649aaacc2e8612b603406"
          }
        },
        "ab8aca607aab433a87bcd7766e8d1697": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b882271d03664ae59d80d5f0ac7ded52",
            "placeholder": "​",
            "style": "IPY_MODEL_f4350bd699d446c79bb12905447ee274",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "86f45e52c0a045c9887a484d713ffa08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_661beb7f1bd94c97bb61a77d61af7c81",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b4fa68529e244f2aad905f0e1f217ed",
            "value": 4
          }
        },
        "fed9cf2bcc5d46d2a33adb4d2bd2cdfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e8f8803fac248369c5932771550886d",
            "placeholder": "​",
            "style": "IPY_MODEL_a8fb8762101d4b45b1235421e81f1e5a",
            "value": " 4/4 [00:59&lt;00:00, 14.94s/it]"
          }
        },
        "0ec11e17184649aaacc2e8612b603406": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b882271d03664ae59d80d5f0ac7ded52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4350bd699d446c79bb12905447ee274": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "661beb7f1bd94c97bb61a77d61af7c81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b4fa68529e244f2aad905f0e1f217ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7e8f8803fac248369c5932771550886d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8fb8762101d4b45b1235421e81f1e5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nate-walter/colab-starcoder/blob/main/StarCoder_W_ToT_DataProcsng.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWzwsYdoUi8o",
        "outputId": "c5cb67ce-72f9-41cb-e56f-56ad8117da8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/starCoder_project"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaU33i5tUtwk",
        "outputId": "8f18d03d-e1e9-4a6b-c6a2-6270c8d84c4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/starCoder_project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd starcoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FYt8Ah_Utf6",
        "outputId": "9651c4f8-8a91-4d84-aa7f-39bfd09f4e0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/starCoder_project/starcoder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Uncomment if for some reason ToT repo not still in path /content/drive/MyDrive/starCoder_project/starcoder\n",
        "\n",
        "# !git clone https://github.com/princeton-nlp/tree-of-thought-llm.git"
      ],
      "metadata": {
        "id": "LAztVKm2RgKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet -r requirements.txt   # This allows you to connect with HuggingFace and sign in\n",
        "!pip install --quiet bitsandbytes\n",
        "!pip install --quiet git+https://github.com/huggingface/transformers.git\n",
        "#!pip install --quiet --upgrade transformers # trying to use the github link to see if it works better (2023-09-11 11:45)\n",
        "!pip install --quiet --upgrade accelerate\n",
        "!pip install --quiet sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dwCHjVlUtY3",
        "outputId": "43a7a808-232b-47b1-a9b0-7d94ee9ff7d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.2/251.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQK-oKHRUtWA",
        "outputId": "abf2ad35-bdfe-4990-8396-0f4810f54e77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "    \n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import os\n",
        "import textwrap\n",
        "\n",
        "class FunctionAndClassExtractor:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.functions = []\n",
        "        self.classes = []\n",
        "        self.repo_name = \"tree-of-thought-llm\"  # Hardcoded for now\n",
        "        self.stars_count = 3000  # Hardcoded for now\n",
        "\n",
        "    def extract_from_file(self, filepath):\n",
        "        with open(filepath, 'r') as f:\n",
        "            source = f.read()\n",
        "\n",
        "        tree = ast.parse(source)\n",
        "\n",
        "        for node in ast.walk(tree):\n",
        "            if isinstance(node, ast.FunctionDef):\n",
        "                function = {\n",
        "                    'name': node.name,\n",
        "                    'params': [a.arg for a in node.args.args],\n",
        "                    'body': textwrap.indent('\\n'.join(source.splitlines()[node.lineno-1:node.end_lineno]), '  '),\n",
        "                    'docstring': ast.get_docstring(node),\n",
        "                    'file_path': filepath,\n",
        "                    'repo_name': self.repo_name,\n",
        "                    'stars_count': self.stars_count\n",
        "                }\n",
        "                self.functions.append(function)\n",
        "\n",
        "            elif isinstance(node, ast.ClassDef):\n",
        "                class_name = node.name\n",
        "                methods = []\n",
        "                for class_node in ast.iter_child_nodes(node):\n",
        "                    if isinstance(class_node, ast.FunctionDef):\n",
        "                        method = {\n",
        "                            'name': class_node.name,\n",
        "                            'params': [a.arg for a in class_node.args.args],\n",
        "                            'body': textwrap.indent('\\n'.join(source.splitlines()[node.lineno-1:node.end_lineno]), '  '),\n",
        "                            'docstring': ast.get_docstring(class_node),\n",
        "                            'file_path': filepath,\n",
        "                            'repo_name': self.repo_name,\n",
        "                            'stars_count': self.stars_count\n",
        "                        }\n",
        "                        methods.append(method)\n",
        "\n",
        "                class_info = {\n",
        "                    'name': class_name,\n",
        "                    'methods': methods,\n",
        "                    'docstring': ast.get_docstring(node),\n",
        "                    'repo_name': self.repo_name,\n",
        "                    'stars_count': self.stars_count\n",
        "                }\n",
        "\n",
        "                self.classes.append(class_info)\n",
        "\n",
        "\n",
        "    def process_dir(self, dirpath):\n",
        "        for root, dirs, files in os.walk(dirpath):\n",
        "            for filename in files:\n",
        "                if filename.endswith('.py'):\n",
        "                    self.extract_from_file(os.path.join(root, filename))\n",
        "\n",
        "    def display_collected_data(self):\n",
        "      pass\n",
        "        # print(\"\\nCollected Functions:\") ## Uncomment to see the funcs and classes\n",
        "        # for function in self.functions:\n",
        "        #     print(function)\n",
        "\n",
        "        # print(\"\\nCollected Classes:\")\n",
        "        # for class_info in self.classes:\n",
        "        #     print(class_info)\n",
        "\n",
        "\n",
        "\n",
        "# Initialize the extractor and process directory\n",
        "extractor = FunctionAndClassExtractor()\n",
        "extractor.process_dir('tree-of-thought-llm')\n",
        "#extractor.display_collected_data() ## Uncomment to see print out of funcs and classes gathered\n",
        "\n",
        "\n",
        "# Existing imports and FunctionAndClassExtractor class definition\n",
        "\n",
        "def format_data(functions, classes):\n",
        "    formatted_data = []\n",
        "\n",
        "    for function in functions:\n",
        "        repo_name = function['repo_name']\n",
        "        stars_count = function['stars_count']\n",
        "        file_path = function['file_path']\n",
        "        code_content = function['body']\n",
        "\n",
        "        record = f'<reponame>{repo_name}<gh_stars>{stars_count}<filename>{file_path} {code_content}'\n",
        "        formatted_data.append(record)\n",
        "\n",
        "    for class_info in classes:\n",
        "        repo_name = class_info['repo_name']\n",
        "        stars_count = class_info['stars_count']\n",
        "        class_name = class_info['name']\n",
        "\n",
        "        record = f'<reponame>{repo_name}<gh_stars>{stars_count}<classname>{class_name}'\n",
        "        formatted_data.append(record)\n",
        "\n",
        "    return formatted_data\n",
        "\n",
        "\n",
        "# Format the extracted data\n",
        "formatted_data = format_data(extractor.functions, extractor.classes)\n",
        "\n",
        "# For demonstration, let's print the first few formatted records\n",
        "print(\"First formatted records:\")\n",
        "for record in formatted_data[:1]:\n",
        "    print(record)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQlqoDsFYHIz",
        "outputId": "3034b91f-9942-475e-d585-b78773b438a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First formatted records:\n",
            "<reponame>tree-of-thought-llm<gh_stars>3000<filename>tree-of-thought-llm/run.py   def run(args):\n",
            "      task = get_task(args.task)\n",
            "      logs, cnt_avg, cnt_any = [], 0, 0\n",
            "      if args.naive_run:\n",
            "          file = f'./logs/{args.task}/{args.backend}_{args.temperature}_naive_{args.prompt_sample}_sample_{args.n_generate_sample}_start{args.task_start_index}_end{args.task_end_index}.json'\n",
            "      else:\n",
            "          file = f'./logs/{args.task}/{args.backend}_{args.temperature}_{args.method_generate}{args.n_generate_sample}_{args.method_evaluate}{args.n_evaluate_sample}_{args.method_select}{args.n_select_sample}_start{args.task_start_index}_end{args.task_end_index}.json'\n",
            "      os.makedirs(os.path.dirname(file), exist_ok=True)\n",
            "\n",
            "      for i in range(args.task_start_index, args.task_end_index):\n",
            "          # solve\n",
            "          if args.naive_run:\n",
            "              ys, info = naive_solve(args, task, i) \n",
            "          else:\n",
            "              ys, info = solve(args, task, i)\n",
            "\n",
            "          # log\n",
            "          infos = [task.test_output(i, y) for y in ys]\n",
            "          info.update({'idx': i, 'ys': ys, 'infos': infos, 'usage_so_far': gpt_usage(args.backend)})\n",
            "          logs.append(info)\n",
            "          with open(file, 'w') as f:\n",
            "              json.dump(logs, f, indent=4)\n",
            "        \n",
            "          # log main metric\n",
            "          accs = [info['r'] for info in infos]\n",
            "          cnt_avg += sum(accs) / len(accs)\n",
            "          cnt_any += any(accs)\n",
            "          print(i, 'sum(accs)', sum(accs), 'cnt_avg', cnt_avg, 'cnt_any', cnt_any, '\\n')\n",
            "    \n",
            "      n = args.task_end_index - args.task_start_index\n",
            "      print(cnt_avg / n, cnt_any / n)\n",
            "      print('usage_so_far', gpt_usage(args.backend))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformers datasets"
      ],
      "metadata": {
        "id": "zCYobB66RwrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Initialize the quantized model\n",
        "name = \"bigcode/starcoderbase-7b\"\n",
        "model_8bit = AutoModelForCausalLM.from_pretrained(name, device_map=\"auto\", load_in_8bit=True)\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(name)\n",
        "\n",
        "# Set the padding token if it's not already set\n",
        "if not tokenizer.pad_token:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Freeze all layers\n",
        "for param in model_8bit.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze the last layer (assuming it's the last layer of the decoder, adjust as needed)\n",
        "for param in model_8bit.transformer.h[-1].parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260,
          "referenced_widgets": [
            "1722adf14dcc4d4db3ea659bffef0186",
            "ab8aca607aab433a87bcd7766e8d1697",
            "86f45e52c0a045c9887a484d713ffa08",
            "fed9cf2bcc5d46d2a33adb4d2bd2cdfc",
            "0ec11e17184649aaacc2e8612b603406",
            "b882271d03664ae59d80d5f0ac7ded52",
            "f4350bd699d446c79bb12905447ee274",
            "661beb7f1bd94c97bb61a77d61af7c81",
            "6b4fa68529e244f2aad905f0e1f217ed",
            "7e8f8803fac248369c5932771550886d",
            "a8fb8762101d4b45b1235421e81f1e5a"
          ]
        },
        "id": "g6ZzWAH_W_Jy",
        "outputId": "d7c1ef03-76b9-4f63-8efa-48436353f2ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1722adf14dcc4d4db3ea659bffef0186"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using pad_token, but it is not set yet.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-97523d71bb2d>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Unfreeze the last layer (assuming it's the last layer of the decoder, adjust as needed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_8bit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: only Tensors of floating point and complex dtype can require gradients"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dud7agKwYZm_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}